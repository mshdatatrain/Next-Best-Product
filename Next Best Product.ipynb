{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e685d36",
   "metadata": {},
   "source": [
    "## Problem Statement and Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e313a8c2",
   "metadata": {},
   "source": [
    "Enhancing product utilization and increasing wallet share are critical objectives for financial institutions, representing key performance indicators (KPIs) in a competitive and dynamic market. Acquiring new customers entails significant costs; however, leveraging existing customer relationships to cross-sell additional products offers a more cost-effective strategy. This approach capitalizes on established connections with customers, reducing the need for extensive marketing efforts while fostering engagement and loyalty with the brand.\n",
    "\n",
    "In response to this need, I have developed a data science model tailored to the banking industry, aimed at identifying the next best product for financial institutions to offer their existing customers. By analyzing relevant data sources, such as transaction histories and demographic information, the model predicts optimal product recommendations, aligning with individual customer preferences and financial needs. This proactive approach not only drives revenue growth but also strengthens customer relationships, positioning banks for long-term success in a competitive marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb20eb5",
   "metadata": {},
   "source": [
    "### import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad607b35-6244-4f91-acb5-01bd43810c59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MultiLabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75060d-5983-46ba-98e4-d96a1929c0c5",
   "metadata": {},
   "source": [
    "## Load the Datsetset and Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88820275",
   "metadata": {},
   "source": [
    "The data comes from Santander Bank and spans approximately 1.5 years, starting from January 28, 2015, with monthly records detailing the products each customer possesses, such as \"credit card,\" \"savings account,\" etc.\n",
    "\n",
    "The objective is to forecast the additional products a customer will acquire in the upcoming months, in addition to their current holdings. To streamline the process, only 30,000 records have been utilized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac182ca5-98e5-4b4c-8d45-ac79148fff20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('Dataset', nrows=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed921c63-a6fe-4168-8c92-1e692c980595",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Description of data and name of variable\n",
    "data_desc = pd.read_csv('data_desc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e98fe-ef6a-41e8-97fd-9c8262826071",
   "metadata": {},
   "source": [
    "## Exploring and Cleaning the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83173fba",
   "metadata": {},
   "source": [
    "\n",
    "It is essential to check the data quality before performing any analyses. The first thing I would like to do is to check the percentage of missing data in each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad788686-3774-46e3-8808-ed8f2a8ce2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Checkign the percentages of missing value in each record\n",
    "raw_df.isnull().mean() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bbc7f5",
   "metadata": {},
   "source": [
    "\n",
    "It appears that there are several records with significant missing values. Therefore, I would like to drop the records that have more than 90% missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd90e805-3257-4d42-861f-03119000b068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop record with more than 90% of missing value\n",
    "data = raw_df.dropna(axis=1, thresh=0.90 *len(raw_df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d36fbdf",
   "metadata": {},
   "source": [
    "\n",
    "There are two common data types in the dataset: categorical and continuous (numerical). To facilitate preprocessing, I will specify and group each data type in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fab83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specificying Data Type\n",
    "categorical_cols = ['CustomerLocation','Gender','CustomerIndex', 'FirstPrimaryCustomer','RelationType','ResidenceIndex', 'IndexJoSed','ChannelUsed','DeceasedIndex', 'ProvinceCode','ActivityIndex','SegmentID']\n",
    "numerical_cols = ['Age','Tenure', 'HGrossIcome',]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37033ebe",
   "metadata": {},
   "source": [
    "The dataset includes information on the products utilized or the list of products used by the consumer. It would be beneficial to identify those variables as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cad3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specificying list of products\n",
    "products = data.iloc[:, 22:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb35659",
   "metadata": {},
   "source": [
    "It appears that there are records where the consumer has not utilized any product. Since our focus is mainly on product utilization, next I would drop those records where product utilizations are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d84c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop records where product usage is missing\n",
    "data = data.dropna(subset=products.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad96eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'NA' with NaN\n",
    "data.replace(' NA', np.nan, inplace=True)\n",
    "\n",
    "# Handle missing values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df67494",
   "metadata": {},
   "outputs": [],
   "source": [
    "products = data.iloc[:, 22:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d50cdf-e297-4145-872d-c288d11d9530",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check data after droping the missing value \n",
    "data.isnull().mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6464df0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize Produt Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e47a7",
   "metadata": {},
   "source": [
    "Visual representation of product usage provides a clear and intuitive way to understand customer behavior and preferences. \n",
    "To create the plot, the code sets the graph style and initializes an empty DataFrame to store the counts of product usage. It then iterates over each product column, counting occurrences of '1', which indicate product usage. These counts are aggregated and stored in the DataFrame, along with the corresponding product column name.\n",
    "After calculating the total count for each product, the DataFrame is sorted by the total count in descending order to prioritize the most frequently used products. The bar plot is then generated using seaborn, with the x-axis representing the products and the y-axis representing the count of product usage. Additionally, the hue parameter is utilized to distinguish between products that are used ('1') and not used ('0').\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3bb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set graph style\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Empty DataFrame to store the counts\n",
    "counts = pd.DataFrame(columns=['column', 'count'])\n",
    "\n",
    "# Iterate over each column and count occurrences of '1'\n",
    "for column_name, column_data in products.items():\n",
    "    # Convert the column data to Series\n",
    "    column_data_series = pd.Series(column_data)\n",
    "    column_counts = column_data_series.value_counts().reset_index()\n",
    "    column_counts.columns = ['value', 'count']\n",
    "    # Append the column index as the 'column' value\n",
    "    column_counts['column'] = column_name\n",
    "    counts = pd.concat([counts, column_counts[column_counts['value'] == 1]], ignore_index=True)\n",
    "\n",
    "# Calculate the total count for each product\n",
    "total_counts = counts.groupby('column')['count'].sum().reset_index()\n",
    "\n",
    "# Sort the DataFrame by the total count in descending order\n",
    "total_counts = total_counts.sort_values(by='count', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "ax = sns.barplot(x=counts['column'], y=counts['count'], hue=counts['value'], order=total_counts['column'])\n",
    "ax.set_title(\"Product Usage\")\n",
    "ax.set_xlabel(\"Products\")\n",
    "ax.set_ylabel(\"Count of Product Used\")\n",
    "ax.legend(title=\"Product used\", title_fontsize=\"14\")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bedb56",
   "metadata": {},
   "source": [
    "## Fiting Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53940fe8",
   "metadata": {},
   "source": [
    "\n",
    "The objective is to forecast the subsequent optimal product. The target encompasses a list of products, while the features entail the customer characteristics. To accomplish this objective, the XGBoost Multiclass Classifier emerges as the most fitting model. This classifier is adept at handling multiple classes and is well-suited to the task of predicting the next best product based on diverse customer attributes. With its robust performance and ability to handle complex data relationships, the XGBoost Multiclass Classifier stands poised to provide accurate predictions, thereby facilitating informed decision-making in product recommendation systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502affc4",
   "metadata": {},
   "source": [
    "\n",
    "The first step is to specify the target and the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3d8f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features and labels\n",
    "X = data[categorical_cols + numerical_cols]\n",
    "y = products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b22c0e",
   "metadata": {},
   "source": [
    "Following the specification of the target and features, the subsequent crucial step involves partitioning the dataset into training and test sets. This partitioning ensures the model's ability to generalize well to unseen data. Typically, a portion of the dataset, usually around 70-80%, is allocated for training the model, while the remaining portion, around 20-30%, is reserved for evaluating its performance. This segregation helps assess the model's effectiveness in predicting new instances accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd3e4a-fefe-45e0-bf14-3e2ea7f0e3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2842cf",
   "metadata": {},
   "source": [
    "### Defining Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06a51fd",
   "metadata": {},
   "source": [
    "The pipeline is defined to preprocess both numerical and categorical columns efficiently. For numerical columns, missing values are imputed using the median as the strategy, followed by scaling through standardization. Meanwhile, for categorical columns, missing values are imputed using the most frequent value strategy, and then one-hot encoding is applied, with the ability to handle unknown categories. These steps are consolidated using the ColumnTransformer.\n",
    "\n",
    "The preprocessing pipeline is integrated into the overall pipeline, which includes a MultiOutputClassifier utilizing the XGBoost algorithm. This classifier is capable of handling multiple classes, making it suitable for the task at hand. The full pipeline encompasses the preprocessing steps and the classifier.\n",
    "\n",
    "To train the model, the pipeline is fitted to the training data. This process ensures that the model learns from the training set while incorporating the preprocessing steps to handle various data types effectively. Upon completion of the fitting process, the pipeline is ready to make predictions on new data, providing insights into the next best product based on customer characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f513d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing steps for numerical and categorical columns\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps for numerical and categorical columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Construct the full pipeline with MultiOutputClassifier and XGBoost\n",
    "classifier = MultiOutputClassifier(XGBClassifier())\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a6ea8",
   "metadata": {},
   "source": [
    "After making predictions using the defined pipeline, the next step involves evaluating the model's performance metrics such as precision and recall. Precision measures the proportion of correctly predicted positive cases out of all predicted positive cases, while recall calculates the proportion of correctly predicted positive cases out of all actual positive cases. These metrics are crucial for assessing the model's ability to accurately classify each label.\n",
    "belo, the predictions are made on the test data using the pipeline. Subsequently, precision and recall scores are computed for each label using the precision_score and recall_score functions, respectively, with the 'micro' averaging strategy. 'Micro' averaging calculates metrics globally by considering each element of the label indicator matrix as an individual data point.\n",
    "Finally, the computed precision and recall scores are printed to provide insights into the model's performance. These metrics offer valuable information about the model's effectiveness in predicting the next best product based on customer characteristics. Higher precision and recall scores indicate better performance, reflecting the model's ability to make accurate predictions while minimizing false positives and false negatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9b2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# precision and recall for each label\n",
    "precision = precision_score(y_test, y_pred, average='micro')  \n",
    "recall = recall_score(y_test, y_pred, average='micro')  \n",
    "\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cfa873",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2c31fd",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is a crucial step in optimizing the performance of machine learning models. It involves systematically searching through a range of hyperparameters to find the combination that yields the best results. In the context of XGBoost, various hyperparameters such as learning rate, number of estimators, maximum depth, and minimum child weight significantly impact the model's performance.\n",
    "I have used grid search, a parameter grid for XGBoost is defined, encompassing different values for these hyperparameters. The grid serves as a blueprint for exploring various combinations during the tuning process. Additionally, scoring metrics such as precision and recall are specified to evaluate the model's performance.\n",
    "To conduct the hyperparameter search, a GridSearchCV object is utilized, iterating over each parameter in the grid. For each parameter, the pipeline is fitted to the training data using cross-validation, and the specified scoring metrics are computed. The best combination of hyperparameters is determined based on the highest precision score, as specified by the 'precision_micro' metric.\n",
    "Throughout the grid search loop, the progress is tracked using tqdm to provide visibility into the process. After completing the search, the best set of parameters and corresponding precision and recall scores are identified. These results offer insights into the optimal configuration for the XGBoost model, ensuring enhanced performance in predicting the next best product based on customer characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0c5283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'classifier__estimator__learning_rate': [0.1, 0.01, 0.001],\n",
    "    'classifier__estimator__n_estimators': [100, 200, 300],\n",
    "    'classifier__estimator__max_depth': [1, 2, 3, 5, 7],\n",
    "    'classifier__estimator__min_child_weight': [1, 3, 5],\n",
    "}\n",
    "\n",
    "# Scoring metrics\n",
    "scoring = {\n",
    "    'precision_micro': make_scorer(precision_score, average='micro'),\n",
    "    'recall_micro': make_scorer(recall_score, average='micro'),\n",
    "}\n",
    "\n",
    "# List to store the results\n",
    "results = []\n",
    "\n",
    "# Grid search loop\n",
    "with tqdm(total=len(param_grid.keys()), desc='Parameter Grid') as pbar:\n",
    "    for params in param_grid.keys():\n",
    "        grid_search = GridSearchCV(pipeline, {params: param_grid[params]}, scoring=scoring, refit='precision_micro', cv=5)\n",
    "        grid_search.fit(X_train, y_train)\n",
    "\n",
    "        # Best parameters and scores\n",
    "        best_params = grid_search.best_params_\n",
    "        precision_micro = grid_search.best_score_\n",
    "        recall_micro = grid_search.cv_results_['mean_test_recall_micro'][grid_search.best_index_]\n",
    "\n",
    "        # Append to the results list\n",
    "        results.append((best_params, precision_micro, recall_micro))\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "# Find the best set of parameters\n",
    "best_results = max(results, key=lambda x: x[1])\n",
    "\n",
    "print(f'Best Parameters: {best_results[0]}')\n",
    "print(f'Precision (micro): {best_results[1]}')\n",
    "print(f'Recall (micro): {best_results[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7937ec7",
   "metadata": {},
   "source": [
    "### Predicting the Next Best Products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642975a4",
   "metadata": {},
   "source": [
    "\n",
    "The next and one of the most import step is predicting the next best product. Below my focuses is on predicting the top three best products for each customer based on their characteristics. Initially, predictions are made using the pipeline, resulting in probability estimates for each product. These probabilities represent the likelihood of a customer purchasing each product.\n",
    "Next, the positive class probabilities are extracted from the predictions, indicating the likelihood of a positive outcome (i.e., purchase) for each product. These probabilities are organized into a DataFrame, with customer IDs as the index and product names as columns.\n",
    "To recommend the top three products for each customer, a loop iterates over the DataFrame, sorting the products based on their probabilities in descending order. The top N recommendations, where N is set to three, are then extracted along with their corresponding probabilities.\n",
    "The recommendations, along with the associated customer IDs and probabilities, are stored in a new DataFrame. This DataFrame provides a clear overview of the top product recommendations for each customer, facilitating targeted marketing efforts and personalized customer experiences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b3752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "predictions_probabilities = np.array(pipeline.predict_proba(X))\n",
    "\n",
    "# Extract the probability of the positive class for each sample\n",
    "positive_class_probabilities = predictions_probabilities[:, :, 1]\n",
    "\n",
    "# 'ID' column is set as the index in the main data\n",
    "results_df = pd.DataFrame(positive_class_probabilities.T, index=X.index, columns=y.columns)\n",
    "\n",
    "# Recommend top 3 products for each customer\n",
    "top_n = 3\n",
    "recommendations = []\n",
    "\n",
    "for customer_id, row in results_df.iterrows():\n",
    "    # Get the top N recommendations with their probabilities\n",
    "    top_recommendations = row.sort_values(ascending=False).head(top_n)\n",
    "    customer_recommendations = list(top_recommendations.index)\n",
    "    probabilities = list(top_recommendations)\n",
    "\n",
    "    recommendations.append((customer_id, customer_recommendations, probabilities))\n",
    "\n",
    "# Create a DataFrame for recommendations\n",
    "columns = ['CustomerID', 'RecommendedProducts', 'Probabilities']\n",
    "recommendations_df = pd.DataFrame(recommendations, columns=columns)\n",
    "\n",
    "# Display or return the recommendations DataFrame\n",
    "recommendations_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec6d63",
   "metadata": {},
   "source": [
    "### Highest Likelihood "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed25064",
   "metadata": {},
   "source": [
    "It's essential to acknowledge that the likelihood of customers adopting a product varies significantly across different individuals. While some customers may have a high probability of using certain products, others may exhibit lower probabilities. Thus, it's valuable to distinguish between customers who are highly likely to adopt a product and those with lower probabilities. This allows for targeted marketing strategies tailored to each customer segment.\n",
    "To highlight these distinctions, would be good to sorts the recommendations DataFrame based on the probability of using the first recommended product. This sorting enables the identification of customers with the highest likelihood of product adoption. By organizing the recommendations in descending order of this probability, customers with the highest potential for product adoption are prioritized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26376f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Highest probabiliteis of use\n",
    "columns = ['CustomerID', 'RecommendedProducts', 'Probabilities']\n",
    "recommendations_df = pd.DataFrame(recommendations, columns=columns)\n",
    "\n",
    "# Sort the DataFrame based on the probability of using the first product\n",
    "recommendations_df['FirstProductProbability'] = recommendations_df['Probabilities'].apply(lambda x: x[0] if x else None)\n",
    "recommendations_df.sort_values(by='FirstProductProbability', ascending=False, inplace=True)\n",
    "\n",
    "# Display or return the sorted recommendations DataFrame\n",
    "recommendations_df.drop(columns='FirstProductProbability', inplace=True)  # Remove the temporary column used for sorting\n",
    "recommendations_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca202c9e",
   "metadata": {},
   "source": [
    "### Lowest Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edef894",
   "metadata": {},
   "source": [
    "To complement the analysis of high likelihoods of product adoption, it's equally important to consider customers with lower probabilities. These customers may require different marketing approaches or additional incentives to encourage product adoption. After sorting the recommendations DataFrame based on the probability of using the first recommended product, the last records are displayed. This action effectively showcases customers with the lowest likelihoods of product adoption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44871bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowesst probabiliteis of use\n",
    "recommendations_df.tail(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
